{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn import cross_validation\n",
    "import xgboost as xgb\n",
    "import operator\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import timeit\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''读入数据并统计读入数据所耗费的时间'''\n",
    "tic0=timeit.default_timer()\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "#%%\n",
    "tic=timeit.default_timer()\n",
    "#读入数据\n",
    "train_orig = pd.read_csv('/Users/Jared/DataAnalysis/Kaggle/Facebook_Checkin/train.csv', header=0)\n",
    "test_orig = pd.read_csv('/Users/Jared/DataAnalysis/Kaggle/Facebook_Checkin/test.csv', header=0)\n",
    "toc=timeit.default_timer()\n",
    "print('Load Time',toc - tic)  #显示数据读入耗费的时间"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "计数place_id并将训练集中的place_id替换为该id的计数\n",
    "创建一个字典，储存每一个place_id的频数：key是place_id，value是频数\n",
    "'''\n",
    "train_orig.sort_values('time',inplace=True)  \n",
    "test_orig.sort_values('time',inplace=True)\n",
    "train_orig_copy = train_orig.copy()\n",
    "train_orig_copy.drop_duplicates('place_id',inplace=True)  #根据place_id去重\n",
    "train_orig_copy['counting'] = 1\n",
    "TEMP_VALUES = train_orig_copy['counting'].cumsum().values\n",
    "\n",
    "PLACE_ID_DICT = pd.Series(TEMP_VALUES,index=train_orig_copy['place_id']).to_dict()  #返回字典，key是place_id，value是计数\n",
    "train_orig_copy['place_id_hash'] = train_orig_copy['place_id'].map(lambda x: PLACE_ID_DICT[x]) \n",
    "REVERSE_PLACE_ID_DICT = dict(zip(train_orig_copy.place_id_hash,train_orig_copy.place_id))  #返回字典，key是计数，value是place_id\n",
    "train_orig['place_id'] = train_orig['place_id'].map(lambda x: PLACE_ID_DICT[x])   #用place_id对应的计数替代id值\n",
    "del train_orig_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def norm_rows(df):\n",
    "    with np.errstate(invalid='ignore'):  #忽略警告提示信息\n",
    "        return df.div(df.sum(axis=1), axis=0).fillna(0)   #df.div 等同于 df/others\n",
    "def get_datetimes(input_df):\n",
    "    input_df['time_dt'] = pd.to_datetime(input_df['time'] * 60, unit='s')  #格式化‘time’列\n",
    "    input_df['hour'] = input_df['time_dt'].map(lambda x: x.hour)   #提取小时\n",
    "    input_df['minute'] = input_df['time_dt'].map(lambda x: x.minute)  #提取分钟\n",
    "    input_df['weekday'] = input_df['time_dt'].map(lambda x: x.weekday())  #提取星期\n",
    "    input_df['month'] = input_df['time_dt'].map(lambda x: x.month)  #提取月份\n",
    "    input_df['year'] = input_df['time_dt'].map(lambda x: x.year - 1970)  #提取年\n",
    "    input_df['day'] = input_df['time_dt'].map(lambda x: x.day)   #提取天\n",
    "    input_df['day_of_year'] = input_df['time_dt'].map(lambda x: x.dayofyear)    #提取一年中的第几天\n",
    "    beginning = pd.to_datetime('1970-01-01 00:00:00')     #设置开始时间\n",
    "    input_df['date_int'] = input_df['time_dt'].map(lambda x: (x - beginning).days)    #计算某一时刻具体开始时间的天数\n",
    "    input_df.drop(['time_dt'],axis=1,inplace=True)     #删除‘time_dt’列\n",
    "    return input_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_top6_places_and_probs(row):\n",
    "    '''返回前6个地点与其对应的概率值'''\n",
    "    row.sort()\n",
    "    inds = row.index[-6:][::-1].tolist()\n",
    "    probs = row[-6:][::-1].tolist()\n",
    "    return inds + probs\n",
    "def create_feature_map(features):\n",
    "    '''将特征输出到外部文件，索引，特征名称'''\n",
    "    outfile = open('xgb.fmap', 'w')\n",
    "    for i, feat in enumerate(features):\n",
    "        outfile.write('{0}\\t{1}\\tq\\n'.format(i, feat))\n",
    "    outfile.close()\n",
    "def apply_dict(common_dict,x,def_val=0):\n",
    "    try:\n",
    "        return common_dict[x]\n",
    "    except KeyError:\n",
    "        return def_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fit_xgb_model(train, test, params, xgb_features, num_rounds = 10, num_boost_rounds = 10,\n",
    "                  use_early_stopping = True, print_feature_imp = False,\n",
    "                  random_seed = 5, reweight_probs = True, calculate_log_loss = True,\n",
    "                  is_sub_run = True):\n",
    "    '''\n",
    "    训练xgb模型\n",
    "    \n",
    "    '''\n",
    "    tic=timeit.default_timer()   #设置计时起点\n",
    "    random.seed(random_seed)     \n",
    "    np.random.seed(random_seed)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    if(use_early_stopping):\n",
    "        '''\n",
    "         如果设置决策树早停，则将从训练数据集中分离出20%的数据作为计算早停的数据集\n",
    "        '''\n",
    "        X_train, X_watch = cross_validation.train_test_split(train, test_size=0.2,random_state=random_seed)  #划分数据\n",
    "        watch_data = X_watch[xgb_features].values\n",
    "        watch_place_id = X_watch['labels'].astype(int).values    #提取验证数据集的label列，并数据类型转换为整形\n",
    "        dwatch = xgb.DMatrix(watch_data, watch_place_id)   #生成xgb验证数据集\n",
    "        train_data = X_train[xgb_features].values\n",
    "        train_place_id = X_train['labels'].astype(int).values \n",
    "        dtrain = xgb.DMatrix(train_data, train_place_id)   #生成xgb训练数据集\n",
    "        watchlist = [(dtrain, 'train'),(dwatch, 'watch')]    #将训练数据集和验证数据集储存到一个列表中（列表中包含两个元组）\n",
    "    else:\n",
    "        '''\n",
    "          不设置早停，即不限制决策树的生长\n",
    "        '''\n",
    "        train_data_full = train[xgb_features].values\n",
    "        train_place_id_full = train['labels'].astype(int).values\n",
    "\n",
    "        weights = (train['date_int']) + 1000\n",
    "        weights = weights / weights.mean()\n",
    "        dtrain_full = xgb.DMatrix(train_data_full, train_place_id_full, weight = weights)\n",
    "\n",
    "\n",
    "    test_data = test[xgb_features].values\n",
    "    dtest = xgb.DMatrix(test_data)   #生成测试数据集\n",
    "\n",
    "    if (use_early_stopping):\n",
    "        xgb_classifier = xgb.train(params, dtrain, num_boost_round=num_rounds, evals=watchlist,\n",
    "                            early_stopping_rounds=10, verbose_eval=50)  #训练xgb分类模型\n",
    "        y_pred = xgb_classifier.predict(dtest,ntree_limit=xgb_classifier.best_iteration)  #返回预测值\n",
    "    else:\n",
    "        xgb_classifier = xgb.train(params, dtrain_full, num_boost_round=num_boost_rounds, evals=[(dtrain_full,'train')],\n",
    "                            verbose_eval=50)   #训练xgb分类模型\n",
    "        y_pred = xgb_classifier.predict(dtest)    #返回预测值\n",
    "\n",
    "    if(print_feature_imp):\n",
    "        create_feature_map(xgb_features)   #将特征子集输出到外部文件\n",
    "        imp_dict = xgb_classifier.get_fscore(fmap='xgb.fmap') \n",
    "        imp_dict = sorted(imp_dict.items(), key=operator.itemgetter(1),reverse=True) \n",
    "        print('{0:<20} {1:>5}'.format('Feature','Imp'))\n",
    "        print(\"--------------------------------------\")\n",
    "        num_to_print = 30\n",
    "        num_printed = 0\n",
    "        #打印前30个特征的特征值及其对应的**值\n",
    "        for i in imp_dict:   \n",
    "            num_printed = num_printed + 1\n",
    "            if (num_printed > num_to_print):\n",
    "                continue\n",
    "            print ('{0:20} {1:5.0f}'.format(i[0], i[1]))\n",
    "    result_xgb_df = pd.DataFrame(index=test.row_id,data=y_pred)   #返回dataframe对象-储存每一行的预测值\n",
    "    result_xgb_df['pred'] = result_xgb_df.apply(get_top6_places_and_probs,axis=1)   #返回概率值最大的6个地点及其对应的概率值  \n",
    "    #提取预测的地点与对应的概率值\n",
    "    result_xgb_df['pred_0'] = result_xgb_df['pred'].map(lambda x: x[0])\n",
    "    result_xgb_df['pred_1'] = result_xgb_df['pred'].map(lambda x: x[1])\n",
    "    result_xgb_df['pred_2'] = result_xgb_df['pred'].map(lambda x: x[2])\n",
    "    result_xgb_df['pred_3'] = result_xgb_df['pred'].map(lambda x: x[3])\n",
    "    result_xgb_df['pred_4'] = result_xgb_df['pred'].map(lambda x: x[4])\n",
    "    result_xgb_df['pred_5'] = result_xgb_df['pred'].map(lambda x: x[5])\n",
    "\n",
    "    result_xgb_df['prob_0'] = result_xgb_df['pred'].map(lambda x: x[6])\n",
    "    result_xgb_df['prob_1'] = result_xgb_df['pred'].map(lambda x: x[7])\n",
    "    result_xgb_df['prob_2'] = result_xgb_df['pred'].map(lambda x: x[8])\n",
    "    result_xgb_df['prob_3'] = result_xgb_df['pred'].map(lambda x: x[9])\n",
    "    result_xgb_df['prob_4'] = result_xgb_df['pred'].map(lambda x: x[10])\n",
    "    result_xgb_df['prob_5'] = result_xgb_df['pred'].map(lambda x: x[11])\n",
    "    result_xgb_df.drop('pred',axis=1,inplace=True)\n",
    "\n",
    "    if(is_sub_run):\n",
    "        print('creating xgb output')\n",
    "        result_xgb_df.reset_index('row_id',inplace=True)\n",
    "    else:\n",
    "        print('cv')\n",
    "        result_xgb_df.reset_index('row_id',inplace=True)\n",
    "        result_xgb_df = pd.merge(result_xgb_df,test[['row_id','place_id','labels']],left_on = ['row_id'],\n",
    "                               right_on = ['row_id'],how='left')\n",
    "    toc=timeit.default_timer()\n",
    "    print('xgb Time',toc - tic)\n",
    "    return result_xgb_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_orig_3 = pd.read_csv('/Users/Jared/DataAnalysis/Kaggle/Facebook_Checkin/train_orig_3.csv', header=0)\n",
    "test_orig_3 = pd.read_csv('/Users/Jared/DataAnalysis/Kaggle/Facebook_Checkin/test_orig_3.csv', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "is_sub_run = True\n",
    "if(is_sub_run):\n",
    "    TRAIN = train_orig_3.copy()\n",
    "    TEST = test_orig_3.copy()\n",
    "else:\n",
    "    time_cond = train_orig_3['time'] <= 600000\n",
    "    TRAIN = train_orig_3[time_cond]\n",
    "    TEST = train_orig_3.loc[~time_cond & (train_orig_3['time'] < 786240)]\n",
    "    train_place_set = set(TRAIN.place_id.unique())\n",
    "    TEST['unique_place'] = TEST['place_id'].map(lambda x: x in train_place_set)\n",
    "    TEST = TEST[TEST['unique_place']]\n",
    "\n",
    "del train_orig_3\n",
    "del test_orig_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "is_sub_run = True\n",
    "if(is_sub_run):\n",
    "    TRAIN = train_orig_3.copy()\n",
    "    TEST = test_orig_3.copy()\n",
    "else:\n",
    "    time_cond = train_orig_3['time'] <= 600000\n",
    "    TRAIN = train_orig_3[time_cond]\n",
    "    TEST = train_orig_3.loc[~time_cond & (train_orig_3['time'] < 786240)]\n",
    "    train_place_set = set(TRAIN.place_id.unique())\n",
    "    TEST['unique_place'] = TEST['place_id'].map(lambda x: x in train_place_set)\n",
    "    TEST = TEST[TEST['unique_place']]\n",
    "\n",
    "del train_orig_3\n",
    "del test_orig_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TRAIN['hour_shift'] = TRAIN['hour'].map(lambda x: x if x >= 12 else x + 24)\n",
    "TRAIN['hour_shift_1'] = TRAIN['hour'].map(lambda x: x if x >= 6 else x + 24)\n",
    "TRAIN['hour_shift_2'] = TRAIN['hour'].map(lambda x: x if x >= 18 else x + 24)\n",
    "TRAIN['hour_and_minute'] = TRAIN['hour'] + TRAIN['minute'] / 60\n",
    "TRAIN['hour_shift_and_minute'] = TRAIN['hour_shift'] + TRAIN['minute'] / 60\n",
    "TRAIN['hour_shift_1_and_minute'] = TRAIN['hour_shift_1'] + TRAIN['minute'] / 60\n",
    "TRAIN['hour_shift_2_and_minute'] = TRAIN['hour_shift_2'] + TRAIN['minute'] / 60\n",
    "\n",
    "TEST['hour_shift'] = TEST['hour'].map(lambda x: x if x >= 12 else x + 24)\n",
    "TEST['hour_shift_1'] = TEST['hour'].map(lambda x: x if x >= 6 else x + 24)\n",
    "TEST['hour_shift_2'] = TEST['hour'].map(lambda x: x if x >= 18 else x + 24)\n",
    "TEST['hour_and_minute'] = TEST['hour'] + TEST['minute'] / 60\n",
    "TEST['hour_shift_and_minute'] = TEST['hour_shift'] + TEST['minute'] / 60\n",
    "TEST['hour_shift_1_and_minute'] = TEST['hour_shift_1'] + TEST['minute'] / 60\n",
    "TEST['hour_shift_2_and_minute'] = TEST['hour_shift_2'] + TEST['minute'] / 60\n",
    "\n",
    "TRAIN.drop(['minute','day','day_of_year'],axis=1,inplace=True)\n",
    "TEST.drop(['minute','day','day_of_year'],axis=1,inplace=True)\n",
    "\n",
    "PLACE_DICT = TRAIN['place_id'].value_counts().to_dict()\n",
    "TRAIN['place_freq_all'] = TRAIN['place_id'].map(lambda x: PLACE_DICT[x])\n",
    "del PLACE_DICT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_probs(df,prob_name):\n",
    "    '''从外部文件中读取地点对应的概率值，将其存入DataFrame对应中'''\n",
    "    temp_probs = pd.read_csv(prob_name, header=0)\n",
    "    for index,row in temp_probs.iterrows():\n",
    "        col_name = 'pred_cv_' + str(row['place'])\n",
    "        col_value = row['pred_place_prob']\n",
    "        df[col_name] = col_value\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_xgb(train,test,output_name = 'res.csv',is_sub_run=True,is_xgb_run = True):\n",
    "\n",
    "    time_max_dict = train.groupby('place_id')['time'].max().to_dict()\n",
    "    time_min_dict = train.groupby('place_id')['time'].min().to_dict()\n",
    "    time_mean_dict = train.groupby('place_id')['time'].mean().to_dict()\n",
    "    train['max_time'] = train['place_id'].map(time_max_dict)\n",
    "    train['min_time'] = train['place_id'].map(time_min_dict)\n",
    "    train['mean_time'] = train['place_id'].map(time_mean_dict)\n",
    "    if(is_xgb_run or compute_knn_probs):\n",
    "        temp_probs = pd.read_csv(prob_name, header=0)\n",
    "        temp_probs_0 = pd.read_csv(prob_name_0, header=0)\n",
    "        temp_probs_1 = pd.read_csv(prob_name_1, header=0)\n",
    "        temp_probs_2 = pd.read_csv(prob_name_2, header=0)\n",
    "        temp_probs_3 = pd.read_csv(prob_name_3, header=0)\n",
    "        temp_dict = pd.Series(temp_probs.place_prob_ratio.values,index=temp_probs.place).to_dict()\n",
    "        temp_dict_0 = pd.Series(temp_probs_0.place_prob_ratio.values,index=temp_probs_0.place).to_dict()\n",
    "        temp_dict_1 = pd.Series(temp_probs_1.place_prob_ratio.values,index=temp_probs_1.place).to_dict()\n",
    "        temp_dict_2 = pd.Series(temp_probs_2.place_prob_ratio.values,index=temp_probs_2.place).to_dict()\n",
    "        temp_dict_3 = pd.Series(temp_probs_3.place_prob_ratio.values,index=temp_probs_3.place).to_dict()\n",
    "        temp_max_dict = pd.Series(temp_probs['pred_place_prob_max'].values,index=temp_probs.place).to_dict()\n",
    "        temp_prob_dict = pd.Series(temp_probs['pred_place_prob'].values,index=temp_probs.place).to_dict()\n",
    "        train['pred_ratio'] = train['place_id'].map(lambda x: apply_dict(temp_dict,x,1))\n",
    "        train['pred_ratio_0'] = train['place_id'].map(lambda x: apply_dict(temp_dict_0,x,1))\n",
    "        train['pred_ratio_1'] = train['place_id'].map(lambda x: apply_dict(temp_dict_1,x,1))\n",
    "        train['pred_ratio_2'] = train['place_id'].map(lambda x: apply_dict(temp_dict_2,x,1))\n",
    "        train['pred_ratio_3'] = train['place_id'].map(lambda x: apply_dict(temp_dict_3,x,1))\n",
    "\n",
    "        train['pred_ratio'] = train['place_id'].map(lambda x: apply_dict(temp_dict,x,1))\n",
    "        train['pred_max'] = train['place_id'].map(lambda x: apply_dict(temp_max_dict,x,1))\n",
    "        train['pred_prob'] = train['place_id'].map(lambda x: apply_dict(temp_prob_dict,x,1))\n",
    "    time_cutoff_percent = 0.8\n",
    "    time_loc_percent = 0.0\n",
    "    time_high_mean_time_percent = 0.75\n",
    "    time_high_min_time_percent = 0.9\n",
    "    time_low_mean_time_percent = 0.25\n",
    "    if(is_sub_run):\n",
    "        time_test = 786240\n",
    "    else:\n",
    "        time_test = 600001\n",
    "    time_cutoff = time_cutoff_percent * time_test\n",
    "    time_high_cutoff = time_loc_percent * time_test\n",
    "    time_high_mean_time = time_high_mean_time_percent * time_test\n",
    "    time_high_min_time = time_high_min_time_percent * time_test\n",
    "    time_low_mean_time = time_low_mean_time_percent * time_test\n",
    "\n",
    "    place_value_counts_dict = train['place_id'].value_counts().to_dict()\n",
    "    train['place_freq'] = train['place_id'].map(lambda x: place_value_counts_dict[x])\n",
    "\n",
    "    if(not is_xgb_run):\n",
    "        train_low_freq_cond = train['place_freq'] <= 3\n",
    "    else:\n",
    "        train_low_freq_cond = train['place_freq'] <= 10\n",
    "        train_high_freq_cond = train['place_freq'] >= 8\n",
    "    train = train[(~train_low_freq_cond)]\n",
    "    if(is_xgb_run):\n",
    "        train = train[train_high_freq_cond]\n",
    "\n",
    "    time_high_cond = train['time'] >= time_high_cutoff\n",
    "    train_high = train[time_high_cond]\n",
    "    high_place_value_counts_dict = train_high['place_id'].value_counts().to_dict()\n",
    "    train['high_time_place_freq'] = train['place_id'].map(lambda x: apply_dict(high_place_value_counts_dict,x,0))\n",
    "\n",
    "    train_low = train[~time_high_cond]\n",
    "    low_place_value_counts_dict = train_low['place_id'].value_counts().to_dict()\n",
    "    train['low_time_place_freq'] = train['place_id'].map(lambda x: apply_dict(low_place_value_counts_dict,x,0))\n",
    "\n",
    "    label_mapping = dict(zip(list(set(train.place_id.values)),range(len(list(set(train.place_id.values))))))\n",
    "    train['labels'] = train['place_id'].map(lambda x: label_mapping[x])\n",
    "    label_id_rev_dict = {v: k for k, v in label_mapping.items()}\n",
    "    if(not is_sub_run):\n",
    "        test['labels'] =  test['place_id'].map(lambda x: apply_dict(label_mapping,x,-1))\n",
    "\n",
    "    low_max_dict = {}\n",
    "    high_min_dict = {}\n",
    "    high_mean_dict = {}\n",
    "    low_mean_dict = {}\n",
    "    temp_dict = {}\n",
    "    temp_dict_0 = {}\n",
    "    temp_dict_1 = {}\n",
    "    temp_dict_2 = {}\n",
    "    temp_dict_3 = {}\n",
    "    train_no_dups = train.drop_duplicates('place_id')\n",
    "    train_no_dups['high_mean_time'] = train_no_dups['mean_time'].map(lambda x: 1.3 if x >= time_high_mean_time else 1.0)\n",
    "    high_mean_dict = pd.Series(train_no_dups.high_mean_time.values,index=train_no_dups['labels'])\n",
    "\n",
    "    train_no_dups['low_mean_time'] = train_no_dups['mean_time'].map(lambda x: 0.5 if x <= time_low_mean_time else 1.0)\n",
    "    low_mean_dict = pd.Series(train_no_dups.low_mean_time.values,index=train_no_dups['labels'])\n",
    "\n",
    "    train_no_dups['high_min_time'] = train_no_dups['min_time'].map(lambda x: 1.3 if x >= time_high_min_time else 1.0)\n",
    "    high_min_dict = pd.Series(train_no_dups.high_min_time.values,index=train_no_dups['labels'])\n",
    "\n",
    "    if(compute_knn_probs):\n",
    "\n",
    "        train_no_dups['low_max_time'] = train_no_dups['max_time'].map(lambda x: 0.5 if x <= time_cutoff else 1.0)\n",
    "        train_no_dups_low_ratio_cond = train_no_dups['pred_ratio'] <= 0.35\n",
    "        train_no_dups['low_max_time'][~train_no_dups_low_ratio_cond] = 1.0\n",
    "        low_max_dict = pd.Series(train_no_dups.low_max_time.values,index=train_no_dups['labels'])\n",
    "        temp_probs = pd.read_csv(prob_name, header=0)\n",
    "        temp_probs['place'] = temp_probs['place'].map(lambda x: apply_dict(label_mapping,x,-1))\n",
    "        place_weight = 0.9\n",
    "        temp_probs['place_prob_ratio'] = temp_probs['place_prob_ratio'] ** place_weight\n",
    "        temp_dict = pd.Series(temp_probs.place_prob_ratio.values,index=temp_probs.place).to_dict()\n",
    "\n",
    "        temp_probs_0 = pd.read_csv(prob_name_0, header=0)\n",
    "        temp_probs_1 = pd.read_csv(prob_name_1, header=0)\n",
    "        temp_probs_2 = pd.read_csv(prob_name_2, header=0)\n",
    "        temp_probs_3 = pd.read_csv(prob_name_3, header=0)\n",
    "\n",
    "        temp_probs_0['place'] = temp_probs_0['place'].map(lambda x: apply_dict(label_mapping,x,-1))\n",
    "        temp_probs_1['place'] = temp_probs_1['place'].map(lambda x: apply_dict(label_mapping,x,-1))\n",
    "        temp_probs_2['place'] = temp_probs_2['place'].map(lambda x: apply_dict(label_mapping,x,-1))\n",
    "        temp_probs_3['place'] = temp_probs_3['place'].map(lambda x: apply_dict(label_mapping,x,-1))\n",
    "\n",
    "        temp_probs_0['place_prob_ratio'] = temp_probs_0['place_prob_ratio'] ** place_weight\n",
    "        temp_probs_1['place_prob_ratio'] = temp_probs_1['place_prob_ratio'] ** place_weight\n",
    "        temp_probs_2['place_prob_ratio'] = temp_probs_2['place_prob_ratio'] ** place_weight\n",
    "        temp_probs_3['place_prob_ratio'] = temp_probs_3['place_prob_ratio'] ** place_weight\n",
    "\n",
    "        temp_dict_0 = pd.Series(temp_probs_0.place_prob_ratio.values,index=temp_probs_0.place).to_dict()\n",
    "        temp_dict_1 = pd.Series(temp_probs_1.place_prob_ratio.values,index=temp_probs_1.place).to_dict()\n",
    "        temp_dict_2 = pd.Series(temp_probs_2.place_prob_ratio.values,index=temp_probs_2.place).to_dict()\n",
    "        temp_dict_3 = pd.Series(temp_probs_3.place_prob_ratio.values,index=temp_probs_3.place).to_dict()\n",
    "\n",
    "    xgb_features = []\n",
    "    xgb_features = (xgb_features + ['x','y','accuracy','weekday','year','month'])\n",
    "    xgb_features = (xgb_features + ['hour_shift_and_minute'])\n",
    "    xgb_features = (xgb_features + ['hour_and_minute'])\n",
    "\n",
    "    knn_features = []\n",
    "    knn_features = (knn_features + ['x','y','weekday','year','month'])\n",
    "    knn_features = (knn_features + ['accuracy'])\n",
    "    knn_features = (knn_features + ['acc_binned'])\n",
    "\n",
    "    if(not is_xgb_run):\n",
    "        weights_dict = {}\n",
    "        if(not compute_knn_probs):\n",
    "            weights_dict['x'] = 500\n",
    "            weights_dict['y'] = 1100\n",
    "            weights_dict['hour_shift'] = 3.5\n",
    "            weights_dict['weekday'] = 3\n",
    "            weights_dict['month'] = 0.3\n",
    "            weights_dict['year'] = 5\n",
    "            weights_dict['accuracy'] = 2.0\n",
    "            weights_dict['acc_binned'] = 2.0\n",
    "            number_neighbors = 45\n",
    "        else:\n",
    "            weights_dict['x'] = 500\n",
    "            weights_dict['y'] = 1100\n",
    "            weights_dict['hour_shift'] = 3.5\n",
    "            weights_dict['weekday'] = 3.0\n",
    "            weights_dict['month'] = 0.2\n",
    "            weights_dict['year'] = 3\n",
    "            weights_dict['accuracy'] = 2.0\n",
    "            weights_dict['acc_binned'] = 2.0\n",
    "            number_neighbors = 50\n",
    "            if(is_sub_run):\n",
    "                number_neighbors = number_neighbors * 1.1\n",
    "        train['x'] = train['x'] * weights_dict['x']\n",
    "        train['y'] = train['y'] * weights_dict['y']\n",
    "        train['hour_and_minute'] = train['hour_and_minute'] * weights_dict['hour_shift']\n",
    "        train['hour_shift_and_minute'] = train['hour_shift_and_minute'] * weights_dict['hour_shift']\n",
    "        train['hour_shift_1_and_minute'] = train['hour_shift_1_and_minute'] * weights_dict['hour_shift']\n",
    "        train['hour_shift_2_and_minute'] = train['hour_shift_2_and_minute'] * weights_dict['hour_shift']\n",
    "        train['weekday'] = train['weekday'] * weights_dict['weekday']\n",
    "        train['month'] = train['month'] * weights_dict['month']\n",
    "        train['year'] = train['year'] * weights_dict['year']\n",
    "        acc_bins = [0,10,30,60,70,100,120,150,180,200,500,10000]\n",
    "        train['acc_binned'] = np.digitize(train['accuracy'],acc_bins,right=True)\n",
    "        train['acc_binned'] = train['acc_binned'] * weights_dict['acc_binned']\n",
    "        train['accuracy'] = np.log10(train['accuracy']) * weights_dict['accuracy']\n",
    "\n",
    "        test['x'] = test['x'] * weights_dict['x']\n",
    "        test['y'] = test['y'] * weights_dict['y']\n",
    "        test['hour_and_minute'] = test['hour_and_minute'] * weights_dict['hour_shift']\n",
    "        test['hour_shift_and_minute'] = test['hour_shift_and_minute'] * weights_dict['hour_shift']\n",
    "        test['hour_shift_1_and_minute'] = test['hour_shift_1_and_minute'] * weights_dict['hour_shift']\n",
    "        test['hour_shift_2_and_minute'] = test['hour_shift_2_and_minute'] * weights_dict['hour_shift']\n",
    "        test['weekday'] = test['weekday'] * weights_dict['weekday']\n",
    "        test['month'] = test['month'] * weights_dict['month']\n",
    "        test['year'] = test['year'] * weights_dict['year']\n",
    "        test['acc_binned'] = np.digitize(test['accuracy'],acc_bins,right=True)\n",
    "        test['acc_binned'] = test['acc_binned'] * weights_dict['acc_binned']\n",
    "        test['accuracy'] = np.log10(test['accuracy']) * weights_dict['accuracy']\n",
    "\n",
    "    number_classes = train['place_id'].nunique()\n",
    "    params = {'learning_rate': 0.05,\n",
    "              'subsample': 0.9,\n",
    "              'reg_alpha': 0.8,\n",
    "#              'lambda': 0.95,\n",
    "              'gamma': 2.0,\n",
    "              'seed': 6,\n",
    "#              'colsample_bytree': 0.8,\n",
    "              'n_estimators': 100,\n",
    "              'objective': 'multi:softprob',\n",
    "              'eval_metric':'mlogloss',\n",
    "              'max_depth': 6,\n",
    "#              'min_child_weight': 2,\n",
    "              'num_class':number_classes}\n",
    "    num_rounds = 10000\n",
    "    if(number_classes > 10):\n",
    "#        result_xgb_df = fit_xgb_model(train,test,params,xgb_features,\n",
    "#                                                       num_rounds = num_rounds, num_boost_rounds = num_rounds,\n",
    "#                                                       use_early_stopping = True, print_feature_imp = False,\n",
    "#                                                       random_seed = 6, is_sub_run = is_sub_run)\n",
    "        if(is_xgb_run):\n",
    "            result_xgb_df = fit_xgb_model(train,test,params,xgb_features,\n",
    "                                  num_rounds = num_rounds, num_boost_rounds = 20,\n",
    "                                  use_early_stopping = False, print_feature_imp = False,random_seed = 6,\n",
    "                                  is_sub_run = is_sub_run)\n",
    "        else:\n",
    "            result_xgb_df = fit_knn_model(train,test,knn_features,\n",
    "                                          num_neighbors = number_neighbors,\n",
    "                                          compute_probs=compute_knn_probs,\n",
    "                                          pred_ratio_dict = temp_dict,\n",
    "                                          pred_ratio_dict_0 = temp_dict_0,\n",
    "                                          pred_ratio_dict_1 = temp_dict_1,\n",
    "                                          pred_ratio_dict_2 = temp_dict_2,\n",
    "                                          pred_ratio_dict_3 = temp_dict_3,\n",
    "                                          low_max_dict = low_max_dict,\n",
    "                                          high_min_dict = high_min_dict,\n",
    "                                          high_mean_dict = high_mean_dict,\n",
    "                                          low_mean_dict = low_mean_dict,\n",
    "                                          is_sub_run = is_sub_run)\n",
    "            if(not compute_knn_probs):\n",
    "                if (not is_sub_run):\n",
    "                    result_xgb_df.drop(['row_id','labels','place_id'],axis=1,inplace=True)\n",
    "                else:\n",
    "                    result_xgb_df.drop(['row_id'],axis=1,inplace=True)\n",
    "                mean_probs = result_xgb_df.mean()\n",
    "                max_probs = result_xgb_df.max()\n",
    "                std_probs = result_xgb_df.std()\n",
    "\n",
    "                mean_probs.name = 'pred_place_prob'\n",
    "                max_probs.name = 'pred_place_prob_max'\n",
    "                std_probs.name = 'pred_place_prob_std'\n",
    "                mean_df = pd.concat([mean_probs,max_probs,std_probs],axis=1)\n",
    "                mean_df.fillna(0,inplace=True)\n",
    "\n",
    "                mean_df.reset_index(inplace=True)\n",
    "                mean_df.rename(columns = {'index':'place'},inplace=True)\n",
    "\n",
    "                place_value_counts_norm_dict = train['place_id'].value_counts(1).to_dict()\n",
    "                mean_df['place'] = mean_df['place'].map(lambda x: label_id_rev_dict[x])\n",
    "                mean_df['train_place_prob'] = mean_df['place'].map(lambda x: place_value_counts_norm_dict[x])\n",
    "                mean_df['place_prob_ratio'] = mean_df['pred_place_prob'] / mean_df['train_place_prob']\n",
    "                mean_df['place_prob_diff'] = mean_df['pred_place_prob'] - mean_df['train_place_prob']\n",
    "                if (not is_sub_run):\n",
    "                    test_place_value_counts_norm_dict = test['place_id'].value_counts(1).to_dict()\n",
    "                    mean_df['test_place_prob'] = mean_df['place'].map(lambda x: apply_dict(test_place_value_counts_norm_dict,x,0))\n",
    "                    mean_df['test_place_prob_ratio'] = mean_df['test_place_prob'] / mean_df['train_place_prob']\n",
    "                mean_df.to_csv(output_name, index=False)\n",
    "                return mean_df\n",
    "        result_xgb_df['pred_0'] = result_xgb_df['pred_0'].map(lambda x: label_id_rev_dict[x])\n",
    "        result_xgb_df['pred_1'] = result_xgb_df['pred_1'].map(lambda x: label_id_rev_dict[x])\n",
    "        result_xgb_df['pred_2'] = result_xgb_df['pred_2'].map(lambda x: label_id_rev_dict[x])\n",
    "        result_xgb_df['pred_3'] = result_xgb_df['pred_3'].map(lambda x: label_id_rev_dict[x])\n",
    "        result_xgb_df['pred_4'] = result_xgb_df['pred_4'].map(lambda x: label_id_rev_dict[x])\n",
    "        result_xgb_df['pred_5'] = result_xgb_df['pred_5'].map(lambda x: label_id_rev_dict[x])\n",
    "\n",
    "        if (is_sub_run):\n",
    "            result_xgb_df = result_xgb_df[['row_id','pred_0','pred_1','pred_2','pred_3','pred_4','pred_5',\n",
    "                               'prob_0','prob_1','prob_2','prob_3','prob_4','prob_5']]\n",
    "        else:\n",
    "            result_xgb_df = result_xgb_df[['row_id','place_id','pred_0','pred_1','pred_2','pred_3','pred_4','pred_5',\n",
    "                               'prob_0','prob_1','prob_2','prob_3','prob_4','prob_5']]\n",
    "\n",
    "        result_xgb_df.to_csv(output_name, index=False)\n",
    "\n",
    "        if(not is_sub_run):\n",
    "            result_xgb_df['pred_0_res'] = result_xgb_df['pred_0'] - result_xgb_df['place_id']\n",
    "            result_xgb_df['pred_1_res'] = result_xgb_df['pred_1'] - result_xgb_df['place_id']\n",
    "            result_xgb_df['pred_2_res'] = result_xgb_df['pred_2'] - result_xgb_df['place_id']\n",
    "            result_xgb_df['pred_0_res'] = (result_xgb_df['pred_0_res'] == 0).astype(int)\n",
    "            result_xgb_df['pred_1_res'] = (result_xgb_df['pred_1_res'] == 0).astype(int) / 2\n",
    "            result_xgb_df['pred_2_res'] = (result_xgb_df['pred_2_res'] == 0).astype(int) / 3\n",
    "            result_xgb_df['apk'] = result_xgb_df['pred_0_res'] + result_xgb_df['pred_1_res'] + result_xgb_df['pred_2_res']\n",
    "            print(output_name)\n",
    "            print('apk',result_xgb_df['apk'].mean())\n",
    "    else:\n",
    "        print(output_name,'less than 10 classes in train')\n",
    "        return 7\n",
    "    return result_xgb_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tic_xgb=timeit.default_timer()\n",
    "if (is_sub_run):\n",
    "    base_str = '/Users/Jared/DataAnalysis/Kaggle/Facebook_Checkin/Test_Results/Run23/'\n",
    "else:\n",
    "    base_str = '/Users/Jared/DataAnalysis/Kaggle/Facebook_Checkin/Train_Results/Run23/'\n",
    "is_xgb_run = False\n",
    "compute_knn_probs = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if(is_xgb_run):\n",
    "    x_range = 40\n",
    "    y_range = 50\n",
    "else:\n",
    "    x_range = 20\n",
    "    y_range = 25\n",
    "for i in range(x_range):\n",
    "    x_width = 0.5\n",
    "    y_width = 0.4\n",
    "    if (is_xgb_run):\n",
    "        extra_edge_x = 0.02\n",
    "        extra_edge_y = 0.02\n",
    "    else:\n",
    "        extra_edge_x = 0.04\n",
    "        extra_edge_y = 0.04\n",
    "    if (i != (x_range - 1)):\n",
    "        train_x = TRAIN.loc[(TRAIN.x < ((i+1)*x_width + extra_edge_x)) & (TRAIN.x >= (i*x_width - extra_edge_x))]\n",
    "        test_x = TEST.loc[(TEST.x < (i+1)*x_width) & (TEST.x >= i*x_width)]\n",
    "    else:\n",
    "        train_x = TRAIN.loc[(TRAIN.x <= ((i+1)*x_width + extra_edge_x)) & (TRAIN.x >= (i*x_width - extra_edge_x))]\n",
    "        test_x = TEST.loc[(TEST.x <= (i+1)*x_width) & (TEST.x >= i*x_width)]\n",
    "    for j in range(y_range):\n",
    "        tic=timeit.default_timer()\n",
    "        if (j != y_range - 1):\n",
    "            cond1_train = train_x.y < (j+1) * y_width + extra_edge_y\n",
    "            cond1_test = test_x.y < (j+1) * y_width\n",
    "        else:\n",
    "            cond1_train = train_x.y <= (j+1) * y_width + extra_edge_y\n",
    "            cond1_test = test_x.y <= (j+1) * y_width\n",
    "        cond2_train = train_x.y >= j*y_width - extra_edge_y\n",
    "        cond2_test = test_x.y >= j*y_width\n",
    "        train_temp = train_x[cond1_train & cond2_train].copy()\n",
    "        test_temp = test_x[cond1_test & cond2_test].copy()\n",
    "        if(is_xgb_run):\n",
    "            name = 'res_x_'+str(i)+'_y_'+str(j)+'.csv'\n",
    "            prob_name = base_str+'prob_x_'+str(i)+'_y_'+str(j)+'.csv'\n",
    "            result_xgb = run_xgb(train_temp.copy(),test_temp.copy(), base_str + name, is_sub_run = is_sub_run,is_xgb_run=is_xgb_run)\n",
    "        else:\n",
    "            if(compute_knn_probs):\n",
    "                name = 'res_x_'+str(i)+'_y_'+str(j)+'.csv'\n",
    "                prob_name = base_str+'prob_x_'+str(i)+'_y_'+str(j)+'.csv'\n",
    "                prob_name_0 = base_str+'prob_x_'+str(i)+'_y_'+str(j)+'_0.csv'\n",
    "                prob_name_1 = base_str+'prob_x_'+str(i)+'_y_'+str(j)+'_1.csv'\n",
    "                prob_name_2 = base_str+'prob_x_'+str(i)+'_y_'+str(j)+'_2.csv'\n",
    "                prob_name_3 = base_str+'prob_x_'+str(i)+'_y_'+str(j)+'_3.csv'\n",
    "                result_xgb = run_xgb(train_temp.copy(),test_temp.copy(), base_str + name, is_sub_run = is_sub_run,is_xgb_run=is_xgb_run)\n",
    "            else:\n",
    "                name = 'prob_x_'+str(i)+'_y_'+str(j)+'.csv'\n",
    "                result_xgb = run_xgb(train_temp.copy(),test_temp.copy(), base_str + name, is_sub_run = is_sub_run,is_xgb_run=is_xgb_run)\n",
    "                name_0 = 'prob_x_'+str(i)+'_y_'+str(j)+'_0.csv'\n",
    "                name_1 = 'prob_x_'+str(i)+'_y_'+str(j)+'_1.csv'\n",
    "                name_2 = 'prob_x_'+str(i)+'_y_'+str(j)+'_2.csv'\n",
    "                name_3 = 'prob_x_'+str(i)+'_y_'+str(j)+'_3.csv'\n",
    "\n",
    "                if(is_sub_run):\n",
    "                    time_min = 786242\n",
    "                    time_max = 1006589\n",
    "                else:\n",
    "                    time_min = 600001\n",
    "                    time_max = 786239\n",
    "                time_diff = time_max - time_min\n",
    "\n",
    "                test_0_cond = (test_temp.time >= (time_min + 0.0 * time_diff)) & (test_temp.time < (time_min + 0.25 * time_diff))\n",
    "                test_1_cond = (test_temp.time >= (time_min + 0.25 * time_diff)) & (test_temp.time < (time_min + 0.5 * time_diff))\n",
    "                test_2_cond = (test_temp.time >= (time_min + 0.5 * time_diff)) & (test_temp.time < (time_min + 0.75 * time_diff))\n",
    "                test_3_cond = (test_temp.time >= (time_min + 0.75 * time_diff)) & (test_temp.time <= time_max)\n",
    "                test_0 = test_temp[test_0_cond]\n",
    "                test_1 = test_temp[test_1_cond]\n",
    "                test_2 = test_temp[test_2_cond]\n",
    "                test_3 = test_temp[test_3_cond]\n",
    "                result_xgb_0 = run_xgb(train_temp.copy(),test_0.copy(), base_str + name_0, is_sub_run = is_sub_run,is_xgb_run=is_xgb_run)\n",
    "                result_xgb_1 = run_xgb(train_temp.copy(),test_1.copy(), base_str + name_1, is_sub_run = is_sub_run,is_xgb_run=is_xgb_run)\n",
    "                result_xgb_2 = run_xgb(train_temp.copy(),test_2.copy(), base_str + name_2, is_sub_run = is_sub_run,is_xgb_run=is_xgb_run)\n",
    "                result_xgb_3 = run_xgb(train_temp.copy(),test_3.copy(), base_str + name_3, is_sub_run = is_sub_run,is_xgb_run=is_xgb_run)\n",
    "toc=timeit.default_timer()\n",
    "print('XGB time',toc - tic_xgb)\n",
    "tic=timeit.default_timer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
